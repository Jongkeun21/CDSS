{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from PIL import Image as PI\n",
    "\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    LABEL\n",
    "        0 -> NORMAL (3,393)\n",
    "        1 -> PNEUMOTHORAX (2,093)\n",
    "        2 -> PNEUMONIA (4,402)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 500\n",
    "batch_size = 20\n",
    "image_resize = 128\n",
    "min_after_dequeue = 5000\n",
    "capacity = min_after_dequeue + 5 * batch_size\n",
    "image_width = image_resize\n",
    "image_height = image_resize\n",
    "initial_rate = 1e-5\n",
    "label_len = 3\n",
    "\n",
    "home_dir = \"/home2/jupyter_notebook/jongkeun/\"\n",
    "\n",
    "image_file = \"dir_images/for_verifying_training/\"\n",
    "# test_image_file = \"dir_images/190309_test/\"\n",
    "\n",
    "label_file = \"csv/for_verifying_training.csv\"\n",
    "# test_label_file = \"csv/190305_test.csv\"\n",
    "\n",
    "log_dir = os.getcwd() + \"/log_dir/\"\n",
    "log_file = \"verifying.csv\"\n",
    "\n",
    "merge_dir = \"./merge_verifying/\"\n",
    "save_dir = \"verifying/verifying\"\n",
    "pb_ = \"verifying.pb\"\n",
    "txt_ = \"verifying.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "label_dir = home_dir + label_file\n",
    "# test_label_dir = home_dir + test_label_file\n",
    "\n",
    "# label_dir = os.getcwd() + \"/csv/181126.csv\"\n",
    "# test_label_dir = os.getcwd() + \"/csv/181126_test.csv\"\n",
    "\n",
    "f_label = open(label_dir, 'r')\n",
    "# f_test_label = open(test_label_dir, 'r')\n",
    "\n",
    "label_data = f_label.readlines()\n",
    "# test_label_data = f_test_label.readlines()\n",
    "\n",
    "f_label.close()\n",
    "# f_test_label.close()\n",
    "\n",
    "label_array = np.array(np.reshape(label_data, [-1, label_len]), dtype=np.int32)\n",
    "# test_label_array = np.array(np.reshape(test_label_data, [-1, label_len]), dtype=np.int32)\n",
    "\n",
    "print(len(label_array))\n",
    "# print(len(test_label_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image, test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "image_dir = home_dir + image_file\n",
    "# test_image_dir = home_dir + test_image_file\n",
    "\n",
    "# image_dir = os.getcwd() + \"/dir_images/181126/\"\n",
    "# test_image_dir = os.getcwd() + \"/dir_images/181126_test/\"\n",
    "\n",
    "image_list = os.listdir(image_dir)\n",
    "# test_image_list = os.listdir(test_image_dir)\n",
    "\n",
    "image_list = sorted(image_list, key=lambda x: (int(re.sub('\\D','',x)),x))\n",
    "# test_image_list = sorted(test_image_list, key=lambda y: (int(re.sub('\\D','',y)),y))\n",
    "\n",
    "image_array = []\n",
    "# test_image_array = []\n",
    "\n",
    "for i in range(len(image_list)) :\n",
    "    image_list[i] = image_dir + image_list[i]\n",
    "    image_array.append(np.array(PI.open(image_list[i]).resize((image_resize, image_resize)).convert('L')))\n",
    "\n",
    "# for j in range(len(test_image_list)) :\n",
    "#     test_image_list[j] = test_image_dir + test_image_list[j]\n",
    "#     test_image_array.append(np.array(PI.open(test_image_list[j]).resize((image_resize, image_resize)).convert('L')))\n",
    "\n",
    "image_array = np.reshape(image_array, [-1, image_width*image_height])\n",
    "# test_image_array = np.reshape(test_image_array, [-1, image_width*image_height])\n",
    "\n",
    "image_array = np.array(np.reshape(image_array, [-1, image_width, image_height, 1]), dtype=np.float32)\n",
    "# test_image_array = np.array(np.reshape(test_image_array, [-1, image_width, image_height, 1]), dtype=np.float32)\n",
    "\n",
    "print(len(image_array))\n",
    "# print(len(test_image_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feeding value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, image_width, image_height, 1], name='input_image')\n",
    "y_ = tf.placeholder(dtype=tf.float32, shape=[None, label_len], name='input_label')\n",
    "learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n",
    "\n",
    "is_training = tf.placeholder(dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_layer(input_data, size, drop_out, is_training, name_conv2d, name_maxpooling, name_dropout) :\n",
    "    layer = tf.layers.conv2d(input_data, size, [3, 3], padding='SAME', name=name_conv2d)\n",
    "    layer = tf.layers.max_pooling2d(layer, [2, 2], [2, 2], padding='SAME', name=name_maxpooling)\n",
    "    layer = tf.layers.dropout(layer, drop_out, is_training, name=name_dropout)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "# def fully_layer(input_data, size, drop_out, activation, is_training, name_dense, name_dropout) :\n",
    "#     fully_layer = tf.contrib.layers.flatten(input_data)\n",
    "#     fully_layer = tf.layers.dense(fully_layer, size, activation=activation, name=name_dense)\n",
    "#     fully_layer = tf.layers.dropout(fully_layer, drop_out=drop_out, is_training=is_training, name=name_dropout)\n",
    "    \n",
    "#     return fully_layer\n",
    "\n",
    "with tf.device('/gpu:0') :\n",
    "    layer0_1 = tf.layers.conv2d(x, 8, [3,3], padding='SAME', name='layer0_conv2d_1')\n",
    "    layer0_2 = tf.layers.conv2d(layer0_1, 128, [3,3], padding='SAME', name='layer0_conv2d_2')\n",
    "    layer0_3 = tf.layers.conv2d(layer0_2, 256, [3,3], padding='SAME', name='layer0_conv2d_3')\n",
    "    layer0_4 = tf.layers.conv2d(layer0_3, 512, [3,3], padding='SAME', name='layer0_conv2d_4')\n",
    "\n",
    "with tf.device('/gpu:6') :\n",
    "#     layer1 = tf.layers.conv2d(x, 32, [3, 3], name='layer1_conv2d')\n",
    "#     layer1 = tf.layers.max_pooling2d(layer1, [2, 2], [2, 2], name='layer1_max_pooling')\n",
    "#     layer1 = tf.layers.dropout(layer1, 0.7, is_training, name='layer1_dropout')\n",
    "    layer1 = learning_layer(input_data=layer0_4, size=256, drop_out=0.7, is_training=is_training, name_conv2d=\"layer1_conv2d\", \n",
    "                   name_maxpooling=\"layer1_maxpooling\", name_dropout=\"layer1_dropout\")\n",
    "    layer1_1 = tf.layers.conv2d(layer1, 128, [3,3], padding='SAME', name='layer1_conv2d_1')\n",
    "    layer1_2 = tf.layers.conv2d(layer1_1, 256, [3,3], padding='SAME', name='layer1_conv2d_2')\n",
    "    layer1_3 = tf.layers.conv2d(layer1_2, 512, [3,3], padding='SAME', name='layer1_conv2d_3')\n",
    "    layer1_4 = tf.layers.conv2d(layer1_3, 1024, [3,3], padding='SAME', name='layer1_conv2d_4')\n",
    "\n",
    "with tf.device('/gpu:1') :\n",
    "#     layer2 = tf.layers.conv2d(layer1, 64, [3, 3], name='layer2')\n",
    "#     layer2 = tf.layers.max_pooling2d(layer2, [2, 2], [2, 2], name='layer2_max_pooling')\n",
    "#     layer2 = tf.layers.dropout(layer2, 0.7, is_training, name='layer2_dropout')\n",
    "    layer2 = learning_layer(input_data=layer1_4, size=512, drop_out=0.7, is_training=is_training, name_conv2d=\"layer2_conv2d\", \n",
    "                   name_maxpooling=\"layer2_maxpooling\", name_dropout=\"layer2_dropout\")\n",
    "    layer2_1 = tf.layers.conv2d(layer2, 256, [3,3], padding='SAME', name='layer2_conv2d_1')\n",
    "    layer2_2 = tf.layers.conv2d(layer2_1, 512, [3,3], padding='SAME', name='layer2_conv2d_2')\n",
    "    layer2_3 = tf.layers.conv2d(layer2_2, 1024, [3,3], padding='SAME', name='layer2_conv2d_3')\n",
    "    layer2_4 = tf.layers.conv2d(layer2_3, 256, [3,3], padding='SAME', name='layer2_conv2d_4')\n",
    "#     layer2_ = tf.layers.conv2d(layer2, 64, [5,5], padding='SAME', name='layer2_conv2d_2')\n",
    "\n",
    "with tf.device('/gpu:7') :\n",
    "#     layer3 = tf.layers.conv2d(layer2, 128, [3, 3], name='layer3')\n",
    "#     layer3 = tf.layers.max_pooling2d(layer3, [2, 2], [2, 2], name='layer3_max_pooling')\n",
    "#     layer3 = tf.layers.dropout(layer3, 0.7, is_training, name='layer3_dropout')\n",
    "    layer3 = learning_layer(input_data=layer2_4, size=512, drop_out=0.7, is_training=is_training, name_conv2d=\"layer3_conv2d\", \n",
    "                   name_maxpooling=\"layer3_maxpooling\", name_dropout=\"layer3_dropout\")\n",
    "    layer3_1 = tf.layers.conv2d(layer3, 1024, [3,3], padding='SAME', name='layer3_conv2d_1')\n",
    "    layer3_2 = tf.layers.conv2d(layer3_1, 512, [3,3], padding='SAME', name='layer3_conv2d_2')\n",
    "    layer3_3 = tf.layers.conv2d(layer3_2, 256, [3,3], padding='SAME', name='layer3_conv2d_3')\n",
    "    layer3_4 = tf.layers.conv2d(layer3_3, 128, [3,3], padding='SAME', name='layer3_conv2d_4')\n",
    "    \n",
    "#     layer3_ = tf.layers.conv2d(layer3, 128, [5,5], padding='SAME', name='layer3_conv2d_2')\n",
    "\n",
    "with tf.device('/gpu:2') :\n",
    "    layer4 = learning_layer(input_data=layer3_4, size=64, drop_out=0.7, is_training=is_training, name_conv2d=\"layer4_conv2d\", \n",
    "                   name_maxpooling=\"layer4_maxpooling\", name_dropout=\"layer4_dropout\")\n",
    "#     layer4_1 = tf.layers.conv2d(layer4, 256, [3,3], padding='SAME', name='layer4_conv2d_1')\n",
    "#     layer4_2 = tf.layers.conv2d(layer4_1, 128, [3,3], padding='SAME', name='layer4_conv2d_2')\n",
    "#     layer4_3 = tf.layers.conv2d(layer4_2, 128, [3,3], padding='SAME', name='layer4_conv2d_3')\n",
    "#     layer4_4 = tf.layers.conv2d(layer4_3, 64, [3,3], padding='SAME', name='layer4_conv2d_4')\n",
    "# #     layer4_ = tf.layers.conv2d(layer4, 64, [5,5], padding='SAME', name='layer4_conv2d_2')\n",
    "\n",
    "#     layer5 = learning_layer(input_data=layer4, size=256, drop_out=0.7, is_training=is_training, name_conv2d=\"layer5_conv2d\", \n",
    "#                    name_maxpooling=\"layer5_maxpooling\", name_dropout=\"layer5_dropout\")\n",
    "# #     layer5_ = tf.layers.conv2d(layer5, 32, [5,5], padding='SAME', name='layer5_conv2d_2')\n",
    "    \n",
    "#     layer6 = learning_layer(input_data=layer5, size=128, drop_out=0.7, is_training=is_training, name_conv2d=\"layer6_conv2d\", \n",
    "#                    name_maxpooling=\"layer6_maxpooling\", name_dropout=\"layer6_dropout\")\n",
    "\n",
    "# with tf.device('/gpu:5') :\n",
    "#     layer5 = learning_layer(input_data=layer4_4, size=512, drop_out=0.7, is_training=is_training, name_conv2d=\"layer5_conv2d\", \n",
    "#                    name_maxpooling=\"layer5_maxpooling\", name_dropout=\"layer5_dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:3') :\n",
    "    fully_layer = tf.contrib.layers.flatten(layer4)\n",
    "    fully_layer = tf.layers.dense(fully_layer, 1024, activation=tf.nn.relu, name='fully_connected_layer')\n",
    "    fully_layer = tf.layers.dropout(fully_layer, 0.5, is_training, name='fully_connected_dropout')\n",
    "#     fully_layer1 = fully_layer(input_data=layer4, size=256, drop_out=0.5, activation=tf.nn.relu, is_training=is_training,\n",
    "#                                name_dense=\"fully_layer1\", name_dropout=\"fully_dropout1\")\n",
    "\n",
    "    fully_layer2 = tf.layers.dense(fully_layer, 512, activation=tf.nn.relu, name='fully_connected_layer2')\n",
    "    fully_layer2 = tf.layers.dropout(fully_layer2, 0.5, is_training, name='fully_connected_dropout2')\n",
    "    \n",
    "    fully_layer3 = tf.layers.dense(fully_layer2, 256, activation=tf.nn.relu, name='fully_connected_layer3')\n",
    "    fully_layer3 = tf.layers.dropout(fully_layer3, 0.5, is_training, name='fully_connected_dropout3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:4') :\n",
    "    logits = tf.layers.dense(fully_layer3, label_len, activation=None)\n",
    "    \n",
    "    with tf.name_scope('pred') : \n",
    "        pred = tf.nn.softmax(logits)\n",
    "    with tf.name_scope('loss') :\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_))\n",
    "#         loss = -tf.reduce_sum(y_ * tf.log(pred))\n",
    "    with tf.name_scope('train') :\n",
    "        train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.device('/gpu:5') :\n",
    "#     correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y_, 1))\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y_, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    label_output = tf.argmax(pred, 1)\n",
    "    label_input = tf.argmax(y_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax_2:0\", shape=(?,), dtype=int64, device=/device:GPU:5)\n",
      "Tensor(\"ArgMax_3:0\", shape=(?,), dtype=int64, device=/device:GPU:5)\n"
     ]
    }
   ],
   "source": [
    "print(label_output)\n",
    "print(label_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.summary.scalar('scalar_loss', loss)\n",
    "\n",
    "image_shaped_input = tf.reshape(x, [-1, image_resize, image_resize, 1])\n",
    "tf.summary.image('input', image_shaped_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name layer0_conv2d_1/kernel:0 is illegal; using layer0_conv2d_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer0_conv2d_1/bias:0 is illegal; using layer0_conv2d_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer0_conv2d_2/kernel:0 is illegal; using layer0_conv2d_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer0_conv2d_2/bias:0 is illegal; using layer0_conv2d_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer0_conv2d_3/kernel:0 is illegal; using layer0_conv2d_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer0_conv2d_3/bias:0 is illegal; using layer0_conv2d_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer0_conv2d_4/kernel:0 is illegal; using layer0_conv2d_4/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer0_conv2d_4/bias:0 is illegal; using layer0_conv2d_4/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer1_conv2d/kernel:0 is illegal; using layer1_conv2d/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer1_conv2d/bias:0 is illegal; using layer1_conv2d/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer1_conv2d_1/kernel:0 is illegal; using layer1_conv2d_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer1_conv2d_1/bias:0 is illegal; using layer1_conv2d_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer1_conv2d_2/kernel:0 is illegal; using layer1_conv2d_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer1_conv2d_2/bias:0 is illegal; using layer1_conv2d_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer1_conv2d_3/kernel:0 is illegal; using layer1_conv2d_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer1_conv2d_3/bias:0 is illegal; using layer1_conv2d_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer1_conv2d_4/kernel:0 is illegal; using layer1_conv2d_4/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer1_conv2d_4/bias:0 is illegal; using layer1_conv2d_4/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer2_conv2d/kernel:0 is illegal; using layer2_conv2d/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer2_conv2d/bias:0 is illegal; using layer2_conv2d/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer2_conv2d_1/kernel:0 is illegal; using layer2_conv2d_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer2_conv2d_1/bias:0 is illegal; using layer2_conv2d_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer2_conv2d_2/kernel:0 is illegal; using layer2_conv2d_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer2_conv2d_2/bias:0 is illegal; using layer2_conv2d_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer2_conv2d_3/kernel:0 is illegal; using layer2_conv2d_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer2_conv2d_3/bias:0 is illegal; using layer2_conv2d_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer2_conv2d_4/kernel:0 is illegal; using layer2_conv2d_4/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer2_conv2d_4/bias:0 is illegal; using layer2_conv2d_4/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer3_conv2d/kernel:0 is illegal; using layer3_conv2d/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer3_conv2d/bias:0 is illegal; using layer3_conv2d/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer3_conv2d_1/kernel:0 is illegal; using layer3_conv2d_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer3_conv2d_1/bias:0 is illegal; using layer3_conv2d_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer3_conv2d_2/kernel:0 is illegal; using layer3_conv2d_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer3_conv2d_2/bias:0 is illegal; using layer3_conv2d_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer3_conv2d_3/kernel:0 is illegal; using layer3_conv2d_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer3_conv2d_3/bias:0 is illegal; using layer3_conv2d_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer3_conv2d_4/kernel:0 is illegal; using layer3_conv2d_4/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer3_conv2d_4/bias:0 is illegal; using layer3_conv2d_4/bias_0 instead.\n",
      "INFO:tensorflow:Summary name layer4_conv2d/kernel:0 is illegal; using layer4_conv2d/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name layer4_conv2d/bias:0 is illegal; using layer4_conv2d/bias_0 instead.\n",
      "INFO:tensorflow:Summary name fully_connected_layer/kernel:0 is illegal; using fully_connected_layer/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name fully_connected_layer/bias:0 is illegal; using fully_connected_layer/bias_0 instead.\n",
      "INFO:tensorflow:Summary name fully_connected_layer2/kernel:0 is illegal; using fully_connected_layer2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name fully_connected_layer2/bias:0 is illegal; using fully_connected_layer2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name fully_connected_layer3/kernel:0 is illegal; using fully_connected_layer3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name fully_connected_layer3/bias:0 is illegal; using fully_connected_layer3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
      "\n",
      "Epoch :     1     Avg. cost =  4.2489\n",
      "Epoch :     2     Avg. cost =  1.3393\n",
      "Epoch :     3     Avg. cost =  1.1075\n",
      "Epoch :     4     Avg. cost =  0.9339\n",
      "Epoch :     5     Avg. cost =  0.9393\n",
      "Epoch :     6     Avg. cost =  0.8597\n",
      "Epoch :     7     Avg. cost =  0.8428\n",
      "Epoch :     8     Avg. cost =  0.8161\n",
      "Epoch :     9     Avg. cost =  0.8025\n",
      "\n",
      "=======================================================================================\n",
      "008:51:35\n",
      "=======================================================================================\n",
      "\n",
      "Epoch :    10     Avg. cost =  0.8011\n",
      "Epoch :    11     Avg. cost =  0.7728\n",
      "Epoch :    12     Avg. cost =  0.7397\n",
      "Epoch :    13     Avg. cost =  0.7867\n",
      "Epoch :    14     Avg. cost =  0.7660\n",
      "Epoch :    15     Avg. cost =  0.7424\n",
      "Epoch :    16     Avg. cost =  0.7364\n",
      "Epoch :    17     Avg. cost =  0.7175\n",
      "Epoch :    18     Avg. cost =  0.7276\n",
      "Epoch :    19     Avg. cost =  0.7351\n",
      "\n",
      "=======================================================================================\n",
      "018:39:54\n",
      "=======================================================================================\n",
      "\n",
      "Epoch :    20     Avg. cost =  0.7203\n",
      "Epoch :    21     Avg. cost =  0.7214\n",
      "Epoch :    22     Avg. cost =  0.6956\n",
      "Epoch :    23     Avg. cost =  0.6825\n",
      "Epoch :    24     Avg. cost =  0.6954\n",
      "Epoch :    25     Avg. cost =  0.7120\n",
      "Epoch :    26     Avg. cost =  0.6884\n",
      "Epoch :    27     Avg. cost =  0.6459\n",
      "Epoch :    28     Avg. cost =  0.6667\n",
      "Epoch :    29     Avg. cost =  0.6321\n",
      "\n",
      "=======================================================================================\n",
      "028:32:36\n",
      "=======================================================================================\n",
      "\n",
      "Epoch :    30     Avg. cost =  0.6255\n",
      "Epoch :    31     Avg. cost =  0.6096\n",
      "Epoch :    32     Avg. cost =  0.6236\n",
      "Epoch :    33     Avg. cost =  0.6249\n",
      "Epoch :    34     Avg. cost =  0.5632\n",
      "Epoch :    35     Avg. cost =  0.5521\n",
      "Epoch :    36     Avg. cost =  0.5488\n",
      "Epoch :    37     Avg. cost =  0.5424\n",
      "Epoch :    38     Avg. cost =  0.5341\n",
      "Epoch :    39     Avg. cost =  0.4775\n",
      "\n",
      "=======================================================================================\n",
      "038:24:36\n",
      "=======================================================================================\n",
      "\n",
      "Epoch :    40     Avg. cost =  0.4997\n",
      "Epoch :    41     Avg. cost =  0.4877\n",
      "Epoch :    42     Avg. cost =  0.4778\n",
      "Epoch :    43     Avg. cost =  0.4987\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7fb54a064b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m                                                                                   is_training: True, learning_rate: initial_rate*rate_})\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mtotal_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1702\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(log_device_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session() as sess :\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    f = open(log_dir+log_file, 'w')\n",
    "    \n",
    "#     saver = tf.train.import_meta_graph('meta_data_name')\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('saved'))\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for var in tf.trainable_variables() :\n",
    "        tf.summary.histogram(var.name, var)\n",
    "\n",
    "    merged_summary = tf.summary.merge_all()  \n",
    "    train_writer = tf.summary.FileWriter(merge_dir, sess.graph)\n",
    "    \n",
    "    total_batch = int(len(image_list) / batch_size)\n",
    "    correct_list = []\n",
    "    incorrect_list = []\n",
    "    \n",
    "#     sensitivity_acc = 0\n",
    "#     specificity_acc = 0\n",
    "    total_acc = 0\n",
    "    rate_ = 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    for epoch in range(training_epochs) :\n",
    "        total_cost = 0\n",
    "        \n",
    "        if epoch%50 == 49 :\n",
    "                rate_ *= 0.8\n",
    "                \n",
    "        if epoch%10 == 9 :\n",
    "#             rate_ *= 0.8\n",
    "            midpoint = int(time.time() - start_time)\n",
    "        \n",
    "            print(\"\")\n",
    "            print(\"=======================================================================================\")\n",
    "            print('{:03d}:{:02d}:{:02d}'.format(midpoint//3600, (midpoint%3600//60), midpoint%60))\n",
    "            print(\"=======================================================================================\")\n",
    "            print(\"\")\n",
    "        \n",
    "        batch_index = np.random.choice(len(image_array), total_batch, replace=False)\n",
    "        \n",
    "        for i in range(total_batch) :                \n",
    "#             _, _loss = sess.run([train, loss], feed_dict={x: image_array[[batch_index[i]]], y_: label_array[[batch_index[i]]], \n",
    "#                                                           is_training: True, learning_rate: initial_rate*rate_})    \n",
    "\n",
    "            _, _loss, _merge = sess.run([train, loss, merged_summary], feed_dict={x: image_array[[batch_index[i]]], y_: label_array[[batch_index[i]]],\n",
    "                                                                                  is_training: True, learning_rate: initial_rate*rate_})\n",
    "            \n",
    "            saver.save(sess, save_dir, global_step=total_batch, write_meta_graph=False)\n",
    "            \n",
    "            total_cost += _loss\n",
    "\n",
    "        avg_cost = total_cost/total_batch\n",
    "        print('Epoch : ', '%4d' % (epoch + 1), '    Avg. cost = ', '{:.4f}'.format(avg_cost))            \n",
    "        \n",
    "        train_writer.add_summary(_merge, epoch)\n",
    "               \n",
    "    print(\"\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"================================     Training done     ================================\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"\")\n",
    "    \n",
    "#     for test in range(len(test_image_array)) :\n",
    "#         _acc, _label_input, _label_output = sess.run([acc, label_input, label_output], \n",
    "#                              feed_dict={x: test_image_array[[test]], y_: test_label_array[[test]], is_training: False})\n",
    "\n",
    "#         total_acc += _acc\n",
    "        \n",
    "#         print(\"n: \", test, \"    label_input: \", _label_input, \"    label_output: \", _label_output)\n",
    "        \n",
    "#         if _label_input == _label_output :\n",
    "#             correct_list.append(test)   \n",
    "#         else :\n",
    "#             incorrect_list.append(test)\n",
    "        \n",
    "    print(\"\")\n",
    "#     print(\"       SENSITIVITY ACC : \", '{:.5f}'.format(sensitivity_acc / 200))\n",
    "#     print(\"       SPECIFICITY ACC : \", '{:.5f}'.format(specificity_acc / 100))\n",
    "    print(\"          TOTAL ACC    : \", '{:.5f}'.format(total_acc / len(test_image_array)))\n",
    "    print(\"\")\n",
    "    \n",
    "#     for cnt in range(len(correct_list)) :\n",
    "#         data = correct_list[cnt]\n",
    "#         f.write(str(data)+\"\\n\")\n",
    "#         print(data)\n",
    "    \n",
    "    tf.train.write_graph(sess.graph, './', pb_, as_text=False)\n",
    "    tf.train.write_graph(sess.graph, './', txt_, as_text=True)\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    \n",
    "    end_time = int(time.time() - start_time)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print('{:03d}:{:02d}:{:02d}'.format(end_time//3600, (end_time%3600//60), end_time%60))\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"\")\n",
    "    \n",
    "#     for cnt_incorrect in range(len(incorrect_list)) :\n",
    "#         print(incorrect_list[cnt_incorrect])\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
