{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from PIL import Image as PI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 500\n",
    "batch_size = 20\n",
    "image_resize = 128\n",
    "min_after_dequeue = 5000\n",
    "capacity = min_after_dequeue + 5 * batch_size\n",
    "image_width = image_resize\n",
    "image_height = image_resize\n",
    "initial_rate = 1e-5\n",
    "label_len = 3\n",
    "\n",
    "home_dir = \"/home2/jupyter_notebook/jongkeun/\"\n",
    "\n",
    "image_file = \"dir_images/for_verifying_training/\"\n",
    "test_image_file = \"dir_images/190309_test/\"\n",
    "\n",
    "label_file = \"csv/for_verifying_training.csv\"\n",
    "test_label_file = \"csv/190305_test.csv\"\n",
    "\n",
    "log_dir = os.getcwd() + \"/log_dir/\"\n",
    "log_file = \"verifying.csv\"\n",
    "\n",
    "merge_dir = \"./merge_verifying/\"\n",
    "save_dir = \"verifying/verifying\"\n",
    "pb_ = \"verifying.pb\"\n",
    "txt_ = \"verifying.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dir = home_dir + label_file\n",
    "test_label_dir = home_dir + test_label_file\n",
    "\n",
    "label_dir = os.getcwd() + \"/csv/181126.csv\"\n",
    "test_label_dir = os.getcwd() + \"/csv/181126_test.csv\"\n",
    "\n",
    "f_label = open(label_dir, 'r')\n",
    "f_test_label = open(test_label_dir, 'r')\n",
    "\n",
    "label_data = f_label.readlines()\n",
    "test_label_data = f_test_label.readlines()\n",
    "\n",
    "f_label.close()\n",
    "f_test_label.close()\n",
    "\n",
    "label_array = np.array(np.reshape(label_data, [-1, label_len]), dtype=np.int32)\n",
    "test_label_array = np.array(np.reshape(test_label_data, [-1, label_len]), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = home_dir + image_file\n",
    "test_image_dir = home_dir + test_image_file\n",
    "\n",
    "image_dir = os.getcwd() + \"/dir_images/181126/\"\n",
    "test_image_dir = os.getcwd() + \"/dir_images/181126_test/\"\n",
    "\n",
    "image_list = os.listdir(image_dir)\n",
    "test_image_list = os.listdir(test_image_dir)\n",
    "\n",
    "image_list = sorted(image_list, key=lambda x: (int(re.sub('\\D','',x)),x))\n",
    "test_image_list = sorted(test_image_list, key=lambda y: (int(re.sub('\\D','',y)),y))\n",
    "\n",
    "image_array = []\n",
    "test_image_array = []\n",
    "\n",
    "for i in range(len(image_list)) :\n",
    "    image_list[i] = image_dir + image_list[i]\n",
    "    image_array.append(np.array(PI.open(image_list[i]).resize((image_resize, image_resize)).convert('L')))\n",
    "\n",
    "for j in range(len(test_image_list)) :\n",
    "    test_image_list[j] = test_image_dir + test_image_list[j]\n",
    "    test_image_array.append(np.array(PI.open(test_image_list[j]).resize((image_resize, image_resize)).convert('L')))\n",
    "\n",
    "image_array = np.reshape(image_array, [-1, image_width*image_height])\n",
    "test_image_array = np.reshape(test_image_array, [-1, image_width*image_height])\n",
    "\n",
    "image_array = np.array(np.reshape(image_array, [-1, image_width, image_height, 1]), dtype=np.float32)\n",
    "test_image_array = np.array(np.reshape(test_image_array, [-1, image_width, image_height, 1]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, image_width, image_height, 1], name='input_image')\n",
    "y_ = tf.placeholder(dtype=tf.float32, shape=[None, label_len], name='input_label')\n",
    "learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n",
    "\n",
    "is_training = tf.placeholder(dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_layer(input_data, size, drop_out, is_training, name_conv2d, name_maxpooling, name_dropout) :\n",
    "    layer = tf.layers.conv2d(input_data, size, [3, 3], padding='SAME', name=name_conv2d)\n",
    "    layer = tf.layers.max_pooling2d(layer, [2, 2], [2, 2], padding='SAME', name=name_maxpooling)\n",
    "    layer = tf.layers.dropout(layer, drop_out, is_training, name=name_dropout)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "with tf.device('/gpu:0') :\n",
    "    layer0_1 = tf.layers.conv2d(x, 8, [3,3], padding='SAME', name='layer0_conv2d_1')\n",
    "    layer0_2 = tf.layers.conv2d(layer0_1, 128, [3,3], padding='SAME', name='layer0_conv2d_2')\n",
    "    layer0_3 = tf.layers.conv2d(layer0_2, 256, [3,3], padding='SAME', name='layer0_conv2d_3')\n",
    "    layer0_4 = tf.layers.conv2d(layer0_3, 512, [3,3], padding='SAME', name='layer0_conv2d_4')\n",
    "\n",
    "with tf.device('/gpu:6') :\n",
    "    layer1 = learning_layer(input_data=layer0_4, size=256, drop_out=0.7, is_training=is_training, name_conv2d=\"layer1_conv2d\", \n",
    "                   name_maxpooling=\"layer1_maxpooling\", name_dropout=\"layer1_dropout\")\n",
    "    layer1_1 = tf.layers.conv2d(layer1, 128, [3,3], padding='SAME', name='layer1_conv2d_1')\n",
    "    layer1_2 = tf.layers.conv2d(layer1_1, 256, [3,3], padding='SAME', name='layer1_conv2d_2')\n",
    "    layer1_3 = tf.layers.conv2d(layer1_2, 512, [3,3], padding='SAME', name='layer1_conv2d_3')\n",
    "    layer1_4 = tf.layers.conv2d(layer1_3, 1024, [3,3], padding='SAME', name='layer1_conv2d_4')\n",
    "\n",
    "with tf.device('/gpu:1') :\n",
    "    layer2 = learning_layer(input_data=layer1_4, size=512, drop_out=0.7, is_training=is_training, name_conv2d=\"layer2_conv2d\", \n",
    "                   name_maxpooling=\"layer2_maxpooling\", name_dropout=\"layer2_dropout\")\n",
    "    layer2_1 = tf.layers.conv2d(layer2, 256, [3,3], padding='SAME', name='layer2_conv2d_1')\n",
    "    layer2_2 = tf.layers.conv2d(layer2_1, 512, [3,3], padding='SAME', name='layer2_conv2d_2')\n",
    "    layer2_3 = tf.layers.conv2d(layer2_2, 1024, [3,3], padding='SAME', name='layer2_conv2d_3')\n",
    "    layer2_4 = tf.layers.conv2d(layer2_3, 256, [3,3], padding='SAME', name='layer2_conv2d_4')\n",
    "\n",
    "with tf.device('/gpu:7') :\n",
    "    layer3 = learning_layer(input_data=layer2_4, size=512, drop_out=0.7, is_training=is_training, name_conv2d=\"layer3_conv2d\", \n",
    "                   name_maxpooling=\"layer3_maxpooling\", name_dropout=\"layer3_dropout\")\n",
    "    layer3_1 = tf.layers.conv2d(layer3, 1024, [3,3], padding='SAME', name='layer3_conv2d_1')\n",
    "    layer3_2 = tf.layers.conv2d(layer3_1, 512, [3,3], padding='SAME', name='layer3_conv2d_2')\n",
    "    layer3_3 = tf.layers.conv2d(layer3_2, 256, [3,3], padding='SAME', name='layer3_conv2d_3')\n",
    "    layer3_4 = tf.layers.conv2d(layer3_3, 128, [3,3], padding='SAME', name='layer3_conv2d_4')\n",
    "    \n",
    "#     layer3_ = tf.layers.conv2d(layer3, 128, [5,5], padding='SAME', name='layer3_conv2d_2')\n",
    "\n",
    "with tf.device('/gpu:2') :\n",
    "    layer4 = learning_layer(input_data=layer3_4, size=64, drop_out=0.7, is_training=is_training, name_conv2d=\"layer4_conv2d\", \n",
    "                   name_maxpooling=\"layer4_maxpooling\", name_dropout=\"layer4_dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:3') :\n",
    "    fully_layer = tf.contrib.layers.flatten(layer4)\n",
    "    fully_layer = tf.layers.dense(fully_layer, 1024, activation=tf.nn.relu, name='fully_connected_layer')\n",
    "    fully_layer = tf.layers.dropout(fully_layer, 0.5, is_training, name='fully_connected_dropout')\n",
    "\n",
    "    fully_layer2 = tf.layers.dense(fully_layer, 512, activation=tf.nn.relu, name='fully_connected_layer2')\n",
    "    fully_layer2 = tf.layers.dropout(fully_layer2, 0.5, is_training, name='fully_connected_dropout2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:4') :\n",
    "    logits = tf.layers.dense(fully_layer3, label_len, activation=None)\n",
    "    \n",
    "    with tf.name_scope('pred') : \n",
    "        pred = tf.nn.softmax(logits)\n",
    "    with tf.name_scope('loss') :\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_))\n",
    "#         loss = -tf.reduce_sum(y_ * tf.log(pred))\n",
    "    with tf.name_scope('train') :\n",
    "        train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.device('/gpu:5') :\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y_, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    label_output = tf.argmax(pred, 1)\n",
    "    label_input = tf.argmax(y_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.summary.scalar('scalar_loss', loss)\n",
    "\n",
    "image_shaped_input = tf.reshape(x, [-1, image_resize, image_resize, 1])\n",
    "tf.summary.image('input', image_shaped_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(log_device_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session() as sess :\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    f = open(log_dir+log_file, 'w')\n",
    "    \n",
    "#     saver = tf.train.import_meta_graph('meta_data_name')\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('saved'))\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for var in tf.trainable_variables() :\n",
    "        tf.summary.histogram(var.name, var)\n",
    "\n",
    "    merged_summary = tf.summary.merge_all()  \n",
    "    train_writer = tf.summary.FileWriter(merge_dir, sess.graph)\n",
    "    \n",
    "    total_batch = int(len(image_list) / batch_size)\n",
    "    correct_list = []\n",
    "    incorrect_list = []\n",
    "    total_acc = 0\n",
    "    rate_ = 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    for epoch in range(training_epochs) :\n",
    "        total_cost = 0\n",
    "        \n",
    "        if epoch%50 == 49 :\n",
    "                rate_ *= 0.8\n",
    "                \n",
    "        if epoch%10 == 9 :\n",
    "#             rate_ *= 0.8\n",
    "            midpoint = int(time.time() - start_time)\n",
    "        \n",
    "            print(\"\")\n",
    "            print(\"=======================================================================================\")\n",
    "            print('{:03d}:{:02d}:{:02d}'.format(midpoint//3600, (midpoint%3600//60), midpoint%60))\n",
    "            print(\"=======================================================================================\")\n",
    "            print(\"\")\n",
    "        \n",
    "        batch_index = np.random.choice(len(image_array), total_batch, replace=False)\n",
    "        \n",
    "        for i in range(total_batch) :                \n",
    "#             _, _loss = sess.run([train, loss], feed_dict={x: image_array[[batch_index[i]]], y_: label_array[[batch_index[i]]], \n",
    "#                                                           is_training: True, learning_rate: initial_rate*rate_})    \n",
    "\n",
    "            _, _loss, _merge = sess.run([train, loss, merged_summary], feed_dict={x: image_array[[batch_index[i]]], y_: label_array[[batch_index[i]]],\n",
    "                                                                                  is_training: True, learning_rate: initial_rate*rate_})\n",
    "            \n",
    "            saver.save(sess, save_dir, global_step=total_batch, write_meta_graph=False)\n",
    "            \n",
    "            total_cost += _loss\n",
    "\n",
    "        avg_cost = total_cost/total_batch\n",
    "        print('Epoch : ', '%4d' % (epoch + 1), '    Avg. cost = ', '{:.4f}'.format(avg_cost))            \n",
    "        \n",
    "        train_writer.add_summary(_merge, epoch)\n",
    "               \n",
    "    print(\"\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"================================     Training done     ================================\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"\")\n",
    "    \n",
    "#     for test in range(len(test_image_array)) :\n",
    "#         _acc, _label_input, _label_output = sess.run([acc, label_input, label_output], \n",
    "#                              feed_dict={x: test_image_array[[test]], y_: test_label_array[[test]], is_training: False})\n",
    "\n",
    "#         total_acc += _acc\n",
    "        \n",
    "#         print(\"n: \", test, \"    label_input: \", _label_input, \"    label_output: \", _label_output)\n",
    "        \n",
    "#         if _label_input == _label_output :\n",
    "#             correct_list.append(test)   \n",
    "#         else :\n",
    "#             incorrect_list.append(test)\n",
    "        \n",
    "    print(\"\")\n",
    "    print(\"          TOTAL ACC    : \", '{:.5f}'.format(total_acc / len(test_image_array)))\n",
    "    print(\"\")\n",
    "    \n",
    "    tf.train.write_graph(sess.graph, './', pb_, as_text=False)\n",
    "    tf.train.write_graph(sess.graph, './', txt_, as_text=True)\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    \n",
    "    end_time = int(time.time() - start_time)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print('{:03d}:{:02d}:{:02d}'.format(end_time//3600, (end_time%3600//60), end_time%60))\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"\")\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(thread)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
